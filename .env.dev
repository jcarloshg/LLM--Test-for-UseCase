# // ─────────────────────────────────────
# OLLAMA
# // ─────────────────────────────────────
OLLAMA_SERVICE_HOST="http://localhost:11435"
OLLAMA_SERVICE_MODEL_LLAMA3-2-1B="llama3.2:1b"
OLLAMA_SERVICE_MODEL_QWEN3VL4B="llama3.2:3b"
OLLAMA_SERVICE_MODEL_QWEN3VL8B="qwen3-vl:8b"

# ─────────────────────────────────────
# Anthopic 
# ─────────────────────────────────────
ANTHOPIC_KEY="ANTHOPIC_KEY"
ANTHOPIC_MODEL="ANTHOPIC_MODEL"

# // ─────────────────────────────────────
# ERROR HANDLING & RETRIES
# // ─────────────────────────────────────
MAX_RETRIES=3
MAX_RETRIES_USER_MSG = "The AI service is currently unavailable. Please try again in a moment."
MAX_RETRIES_DEV_MSG = "Failed to call Ollama. Attemp # "

# // ─────────────────────────────────────
# // NEW VARIABLES
# // ─────────────────────────────────────
# Ollama Configuration
OLLAMA_API_URL=http://localhost:11435
OLLAMA_MODEL=llama3.2:1b

# LLM Settings
TEMPERATURE=0.7
TOP_P=0.9

# Logging
LOG_LEVEL=INFO
DEBUG=false

# Data
DATA_DIR=./data
JSON_FILE_PATH=./data/user_stories_with_test_cases.json
VECTORSTORE_PATH=data/vectorstore_faiss