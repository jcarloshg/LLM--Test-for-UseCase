# Phase 1: Problem Definition & Use Case Design

## Objective

- Based on a story, the microservice must generate structured test cases using a Large Language Model (LLM). Generating the answers by promoting way.

## Key Activities

- Documentar los requisitos y las necesidades de cumplimiento (RGPD, HIPAA)
- Decidir entre construir o comprar (API frente a modelos alojados en el propio servidor)

- Identificar métricas de éxito (precisión, latencia, coste por solicitud)

## Tools

- Requirements documentation: Files markdown on project
- Architecture diagrams: Mermaind
- Decision matrices: **ToDo**

## Outputs

- Requirements document with success criteria: precisión, latencia, coste por solicitud
- Architecture decision record: Microservices

# Matix

### 1. Precisión y Calidad de la Salida (Accuracy & Quality)

Dado que el objetivo es generar artefactos de prueba válidos y estructurados, la precisión no se mide como en un modelo de clasificación tradicional, sino a través del cumplimiento de reglas y la utilidad del texto generado.

- **Tasa de Cumplimiento Estructural (JSON Parsing Success Rate):** Porcentaje de respuestas del LLM que pueden ser parseadas exitosamente como un JSON válido en el primer intento, sin activar el mecanismo de reintento. El objetivo debe apuntar a un >95%.
- **Promedio del `quality_score`:** La API debe devolver un puntaje de calidad. Esta métrica rastrea el promedio de este puntaje a lo largo del tiempo, evaluando heurísticas como: ¿Están vacías las precondiciones? ¿Tienen los casos de prueba un número mínimo de pasos lógicos?
- **Tasa de Reintentos (Retry Rate):** La frecuencia con la que el microservicio debe volver a invocar a Ollama debido a una respuesta malformada o alucinaciones que rompen el esquema. Una tasa alta indica que el diseño del prompt necesita ajustes.

### 2. Latencia y Rendimiento (Latency)

El uso de modelos locales (como Llama 3 o Mistral vía Ollama) introduce desafíos específicos de latencia, especialmente si no se cuenta con aceleración por hardware (GPU) en el entorno de despliegue.

- **Latencia Total del Endpoint (P90 / P99):** El tiempo total transcurrido desde que se recibe el `POST /generate-tests` hasta que se devuelve el payload JSON al cliente. Esto incluye la inferencia, el parseo y las validaciones. Se debe establecer un límite aceptable (ej. < 5 segundos en el percentil 90).
- **Sobrecarga por Validación (Validation Overhead):** El tiempo extra que toma ejecutar el mecanismo de validación (ya sea validación estructural, heurística o autoevaluación mediante un segundo prompt) frente al tiempo puro de generación del texto.

### 3. Coste por Solicitud y Eficiencia (Cost per Request)

Uno de los requerimientos es exponer consideraciones de escalabilidad y costo. Al utilizar Ollama, el costo de inferencia por token de API (como sucedería con OpenAI) es $0. Sin embargo, el costo se traslada a la infraestructura de cómputo.

- **Costo de Infraestructura por 1,000 Solicitudes:** Se calcula dividiendo el costo mensual del servidor (o instancia en la nube) que aloja los contenedores Docker por el rendimiento máximo de solicitudes que la máquina puede procesar sin degradar la latencia.
- **Utilización de Recursos (CPU / RAM / VRAM):** Monitoreo del consumo de memoria y procesamiento del contenedor Docker de Ollama durante la carga máxima. Esto es crucial para justificar trade-offs; por ejemplo, elegir un modelo cuantizado (4-bit) reduce drásticamente el uso de RAM, permitiendo alojar el microservicio en instancias más económicas a costa de una leve pérdida en la precisión del lenguaje.
